{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-containerized-venv-markitdown-scw-fa",
      "display_name": "Python in SCW-FA (env markitdown)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.11.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "modifiedBy": "fabien.antoine@cgi.com",
    "createdOn": 1735315006284,
    "associatedRecipe": "compute_a220_tech_docs_content",
    "creationTag": {
      "versionNumber": 0,
      "lastModifiedBy": {
        "login": "fabien.antoine@cgi.com"
      },
      "lastModifiedOn": 1735315006284
    },
    "customFields": {},
    "tags": [
      "recipe-editor"
    ],
    "dkuGit": {
      "lastInteraction": 0
    },
    "creator": "fabien.antoine@cgi.com"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nimport io\nimport pandas as pd\nimport re\nimport os\nfrom math import ceil\nfrom concurrent.futures import ThreadPoolExecutor\nimport time\n\n# Définir les dossiers d\u0027entrée et de sortie\npdf_folder \u003d dataiku.Folder(\"W8lS5GmB\")  # Dossier contenant les PDF originaux\nmd_folder \u003d dataiku.Folder(\"d7DdDueY\")   # Dossier contenant les annotations générées\n\n# Lister les fichiers PDF\npdf_files \u003d [f for f in pdf_folder.list_paths_in_partition() if f.lower().endswith(\".pdf\")]\npdf_files.sort()\n\n# Taille des lots\nBATCH_SIZE \u003d 100\n# Nombre maximum d\u0027images\nMAX_IMAGES \u003d 9\n\n# Mettre en cache la liste des fichiers du dossier md pour éviter de la récupérer à chaque itération\nall_md_files \u003d set(md_folder.list_paths_in_partition())\n\n# Fonction pour lire le contenu d\u0027un fichier s\u0027il existe\ndef read_file_content(folder, file_path):\n    if file_path in all_md_files:\n        with folder.get_download_stream(file_path) as stream:\n            return io.BytesIO(stream.read()).read().decode(\"utf-8\")\n    return None\n\n# Fonction pour traiter un PDF et créer une ligne de données\ndef process_pdf(pdf_file):\n    base_name \u003d os.path.splitext(pdf_file)[0]\n    doc_root \u003d base_name.split(\u0027_page_\u0027)[0] if \u0027_page_\u0027 in base_name else base_name\n    \n    # Chemins des fichiers d\u0027annotation\n    json_file \u003d base_name + \".json\"\n    md_file \u003d base_name + \".md\"\n    md_img_file \u003d base_name + \"__with_img_desc.md\"\n    json_img_file \u003d base_name + \"__with_img_desc.json\"\n    \n    # Lire le contenu MD et MD_IMG\n    md_content \u003d read_file_content(md_folder, md_file)\n    md_img_content \u003d read_file_content(md_folder, md_img_file)\n    \n    # Si md_img est vide, utiliser md à la place\n    if not md_img_content:\n        md_img_content \u003d md_content\n    \n    # Initialiser les données de la ligne\n    row_data \u003d {\n        \"doc\": pdf_file,\n        \"doc_root\": doc_root + \".pdf\",\n        \"json\": json_file,\n        \"md\": md_content,\n        \"md_img\": md_img_content,\n        \"json_img\": json_img_file\n    }\n    \n    # Initialiser toutes les colonnes d\u0027images à None\n    for i in range(MAX_IMAGES):\n        row_data[f\"img-{i}\"] \u003d None\n        row_data[f\"img-{i}-desc\"] \u003d None\n    \n    # Optimisation: Utiliser un pattern précompilé et filtrer d\u0027abord les fichiers pertinents\n    img_pattern \u003d re.compile(rf\"^{re.escape(base_name)}-img-(\\d+)\\.jpeg$\")\n    \n    # Chercher seulement les fichiers qui pourraient être des images ou des descriptions pour ce PDF\n    base_prefix \u003d f\"{base_name}-img-\"\n    relevant_files \u003d [f for f in all_md_files if f.startswith(base_prefix)]\n    \n    # Traiter les fichiers pertinents\n    for file_path in relevant_files:\n        img_match \u003d img_pattern.match(file_path)\n        if img_match:\n            img_num \u003d int(img_match.group(1))\n            if img_num \u003e\u003d MAX_IMAGES:\n                continue\n            \n            img_key \u003d f\"img-{img_num}\"\n            desc_file \u003d f\"{base_name}-img-{img_num}.md\"\n            \n            row_data[img_key] \u003d file_path\n            if desc_file in all_md_files:\n                row_data[f\"{img_key}-desc\"] \u003d read_file_content(md_folder, desc_file)\n    \n    return row_data\n\n# Accéder au dataset de sortie\noutput_dataset \u003d dataiku.Dataset(\"a220_tech_docs_content\")\n\n# Diviser les fichiers en lots\ntotal_batches \u003d ceil(len(pdf_files) / BATCH_SIZE)\nprint(f\"Traitement de {len(pdf_files)} documents en {total_batches} lots de {BATCH_SIZE} documents.\")\n\n# Définir toutes les colonnes possibles à l\u0027avance\nbase_columns \u003d [\"doc\", \"doc_root\", \"json\", \"md\", \"md_img\", \"json_img\"]\nimage_columns \u003d [f\"img-{i}\" for i in range(MAX_IMAGES)] + [f\"img-{i}-desc\" for i in range(MAX_IMAGES)]\nall_columns \u003d base_columns + image_columns\n\n# Traiter le premier lot pour définir le schéma\nstart_idx \u003d 0\nend_idx \u003d min(BATCH_SIZE, len(pdf_files))\nfirst_batch \u003d pdf_files[start_idx:end_idx]\nprint(f\"Traitement du premier lot : documents {start_idx + 1} à {end_idx}...\")\n\nstart_time \u003d time.time()\n\n# Utiliser le multithreading pour le premier lot\nwith ThreadPoolExecutor(max_workers\u003d8) as executor:\n    results \u003d list(executor.map(process_pdf, first_batch))\n\n# Créer le DataFrame à partir des résultats\ndf_first_batch \u003d pd.DataFrame(results)\n\n# Réorganiser les colonnes pour correspondre à all_columns\ndf_first_batch \u003d df_first_batch[all_columns]\n\n# Définir le schéma à partir du premier lot\noutput_dataset.write_schema_from_dataframe(df_first_batch)\n\n# Ouvrir le writer une seule fois pour tous les lots\nwith output_dataset.get_writer() as writer:\n    # Écrire le premier lot\n    writer.write_dataframe(df_first_batch)\n    first_batch_time \u003d time.time() - start_time\n    print(f\"Lot 1 traité : {len(df_first_batch)} documents ajoutés au dataset en {first_batch_time:.2f} secondes.\")\n    \n    # Traiter les lots restants\n    for batch_num in range(1, total_batches):\n        batch_start_time \u003d time.time()\n        start_idx \u003d batch_num * BATCH_SIZE\n        end_idx \u003d min((batch_num + 1) * BATCH_SIZE, len(pdf_files))\n        current_batch \u003d pdf_files[start_idx:end_idx]\n        print(f\"Traitement du lot {batch_num + 1}/{total_batches}, documents {start_idx + 1} à {end_idx}...\")\n        \n        # Utiliser le multithreading pour traiter le lot courant\n        with ThreadPoolExecutor(max_workers\u003d8) as executor:\n            results \u003d list(executor.map(process_pdf, current_batch))\n        \n        # Créer le DataFrame à partir des résultats\n        df_batch \u003d pd.DataFrame(results)\n        \n        # Réorganiser les colonnes pour correspondre à all_columns\n        df_batch \u003d df_batch[all_columns]\n        \n        # Écrire le lot courant\n        writer.write_dataframe(df_batch)\n        batch_time \u003d time.time() - batch_start_time\n        print(f\"Lot {batch_num + 1} traité : {len(df_batch)} documents ajoutés au dataset en {batch_time:.2f} secondes.\")\n        \n        # Estimer le temps restant\n        avg_time_per_batch \u003d (time.time() - start_time) / (batch_num + 1)\n        remaining_batches \u003d total_batches - (batch_num + 1)\n        estimated_time_remaining \u003d avg_time_per_batch * remaining_batches\n        hours, remainder \u003d divmod(estimated_time_remaining, 3600)\n        minutes, seconds \u003d divmod(remainder, 60)\n        print(f\"Temps restant estimé : {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n\ntotal_time \u003d time.time() - start_time\nhours, remainder \u003d divmod(total_time, 3600)\nminutes, seconds \u003d divmod(remainder, 60)\nprint(f\"Extraction terminée avec succès. Total : {len(pdf_files)} documents traités en {total_batches} lots.\")\nprint(f\"Temps total d\u0027exécution : {int(hours)}h {int(minutes)}m {int(seconds)}s\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}