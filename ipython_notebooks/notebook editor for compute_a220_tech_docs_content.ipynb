{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-containerized-venv-markitdown-scw-fa",
      "display_name": "Python in SCW-FA (env markitdown)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.11.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "modifiedBy": "fabien.antoine@cgi.com",
    "createdOn": 1735315006284,
    "associatedRecipe": "compute_a220_tech_docs_content",
    "creationTag": {
      "versionNumber": 0,
      "lastModifiedBy": {
        "login": "fabien.antoine@cgi.com"
      },
      "lastModifiedOn": 1735315006284
    },
    "customFields": {},
    "tags": [
      "recipe-editor"
    ],
    "dkuGit": {
      "lastInteraction": 0
    },
    "creator": "fabien.antoine@cgi.com"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nimport io\nimport pandas as pd\nimport re\nimport os\nfrom math import ceil\n\n# Définir les dossiers d\u0027entrée et de sortie\npdf_folder \u003d dataiku.Folder(\"W8lS5GmB\")  # Dossier contenant les PDF originaux\nmd_folder \u003d dataiku.Folder(\"d7DdDueY\")   # Dossier contenant les annotations générées\n\n# Lister les fichiers PDF\npdf_files \u003d [f for f in pdf_folder.list_paths_in_partition() if f.lower().endswith(\".pdf\")]\npdf_files.sort()\n\n# Taille des lots\nBATCH_SIZE \u003d 20\n# Nombre maximum d\u0027images\nMAX_IMAGES \u003d 9\n\n# Fonction pour lire le contenu d\u0027un fichier s\u0027il existe\ndef read_file_content(folder, file_path):\n    if file_path in folder.list_paths_in_partition():\n        with folder.get_download_stream(file_path) as stream:\n            return io.BytesIO(stream.read()).read().decode(\"utf-8\")\n    return None\n\n# Accéder au dataset de sortie\noutput_dataset \u003d dataiku.Dataset(\"a220_tech_docs_content\")\n\n# Diviser les fichiers en lots\ntotal_batches \u003d ceil(len(pdf_files) / BATCH_SIZE)\nprint(f\"Traitement de {len(pdf_files)} documents en {total_batches} lots de {BATCH_SIZE} documents.\")\n\n# Définir toutes les colonnes possibles à l\u0027avance\nbase_columns \u003d [\"doc\", \"doc_root\", \"json\", \"md\", \"md_img\", \"json_img\"]\nimage_columns \u003d [f\"img-{i}\" for i in range(MAX_IMAGES)] + [f\"img-{i}-desc\" for i in range(MAX_IMAGES)]\nall_columns \u003d base_columns + image_columns\n\n# Traiter le premier lot pour définir le schéma\nstart_idx \u003d 0\nend_idx \u003d min(BATCH_SIZE, len(pdf_files))\nfirst_batch \u003d pdf_files[start_idx:end_idx]\nprint(f\"Traitement du premier lot : documents {start_idx + 1} à {end_idx}...\")\n\n# Initialiser le dictionnaire pour stocker les données du premier lot\ndata \u003d {col: [] for col in all_columns}\n\n# Pour chaque PDF dans le premier lot, extraire toutes les annotations associées\nfor pdf_file in first_batch:\n    base_name \u003d os.path.splitext(pdf_file)[0]\n    doc_root \u003d base_name.split(\u0027_page_\u0027)[0] if \u0027_page_\u0027 in base_name else base_name\n    \n    # Chemins des fichiers d\u0027annotation\n    json_file \u003d base_name + \".json\"\n    md_file \u003d base_name + \".md\"\n    md_img_file \u003d base_name + \"__with_img_desc.md\"\n    json_img_file \u003d base_name + \"__with_img_desc.json\"\n    \n    # Lire le contenu des fichiers d\u0027annotation\n    row_data \u003d {\n        \"doc\": pdf_file,\n        \"doc_root\": doc_root + \".pdf\",\n        \"json\": json_file,\n        \"md\": read_file_content(md_folder, md_file),\n        \"md_img\": read_file_content(md_folder, md_img_file),\n        \"json_img\": json_img_file\n    }\n    \n    # Initialiser toutes les colonnes d\u0027images à None\n    for col in image_columns:\n        row_data[col] \u003d None\n    \n    # Identifier les images associées au document\n    img_pattern \u003d re.compile(rf\"^{re.escape(base_name)}-img-(\\d+)\\.jpeg$\")\n    all_files \u003d md_folder.list_paths_in_partition()\n    \n    # Chercher toutes les images et leurs descriptions\n    for file_path in all_files:\n        img_match \u003d img_pattern.match(file_path)\n        if img_match:\n            img_num \u003d int(img_match.group(1))\n            if img_num \u003e\u003d MAX_IMAGES:\n                continue\n            img_key \u003d f\"img-{img_num}\"\n            desc_file \u003d f\"{base_name}-img-{img_num}.md\"\n            row_data[img_key] \u003d file_path\n            if desc_file in all_files:\n                row_data[f\"{img_key}-desc\"] \u003d read_file_content(md_folder, desc_file)\n    \n    # Ajouter les données de cette ligne\n    for col in all_columns:\n        data[col].append(row_data.get(col, None))\n\n# Créer le DataFrame du premier lot\ndf_first_batch \u003d pd.DataFrame(data)\n\n# Définir le schéma à partir du premier lot\noutput_dataset.write_schema_from_dataframe(df_first_batch)\n\n# Ouvrir le writer une seule fois pour tous les lots\nwith output_dataset.get_writer() as writer:\n    # Écrire le premier lot\n    writer.write_dataframe(df_first_batch)\n    print(f\"Lot 1 traité : {len(df_first_batch)} documents ajoutés au dataset.\")\n    \n    # Traiter les lots restants\n    for batch_num in range(1, total_batches):\n        start_idx \u003d batch_num * BATCH_SIZE\n        end_idx \u003d min((batch_num + 1) * BATCH_SIZE, len(pdf_files))\n        current_batch \u003d pdf_files[start_idx:end_idx]\n        print(f\"Traitement du lot {batch_num + 1}/{total_batches}, documents {start_idx + 1} à {end_idx}...\")\n        \n        # Initialiser le dictionnaire pour stocker les données du lot\n        data \u003d {col: [] for col in all_columns}\n        \n        # Pour chaque PDF dans le lot, extraire toutes les annotations associées\n        for pdf_file in current_batch:\n            base_name \u003d os.path.splitext(pdf_file)[0]\n            doc_root \u003d base_name.split(\u0027_page_\u0027)[0] if \u0027_page_\u0027 in base_name else base_name\n            \n            # Chemins des fichiers d\u0027annotation\n            json_file \u003d base_name + \".json\"\n            md_file \u003d base_name + \".md\"\n            md_img_file \u003d base_name + \"__with_img_desc.md\"\n            json_img_file \u003d base_name + \"__with_img_desc.json\"\n            \n            # Lire le contenu des fichiers d\u0027annotation\n            row_data \u003d {\n                \"doc\": pdf_file,\n                \"doc_root\": doc_root + \".pdf\",\n                \"json\": json_file,\n                \"md\": read_file_content(md_folder, md_file),\n                \"md_img\": read_file_content(md_folder, md_img_file),\n                \"json_img\": json_img_file\n            }\n            \n            # Initialiser toutes les colonnes d\u0027images à None\n            for col in image_columns:\n                row_data[col] \u003d None\n            \n            # Identifier les images associées au document\n            img_pattern \u003d re.compile(rf\"^{re.escape(base_name)}-img-(\\d+)\\.jpeg$\")\n            all_files \u003d md_folder.list_paths_in_partition()\n            \n            # Chercher toutes les images et leurs descriptions\n            for file_path in all_files:\n                img_match \u003d img_pattern.match(file_path)\n                if img_match:\n                    img_num \u003d int(img_match.group(1))\n                    if img_num \u003e\u003d MAX_IMAGES:\n                        continue\n                    img_key \u003d f\"img-{img_num}\"\n                    desc_file \u003d f\"{base_name}-img-{img_num}.md\"\n                    row_data[img_key] \u003d file_path\n                    if desc_file in all_files:\n                        row_data[f\"{img_key}-desc\"] \u003d read_file_content(md_folder, desc_file)\n            \n            # Ajouter les données de cette ligne\n            for col in all_columns:\n                data[col].append(row_data.get(col, None))\n        \n        # Créer le DataFrame du lot courant\n        df_batch \u003d pd.DataFrame(data)\n        \n        # Écrire le lot courant\n        writer.write_dataframe(df_batch)\n        print(f\"Lot {batch_num + 1} traité : {len(df_batch)} documents ajoutés au dataset.\")\n\nprint(f\"Extraction terminée avec succès. Total : {len(pdf_files)} documents traités en {total_batches} lots vers le dataset \u0027a220_tech_docs_content\u0027.\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}