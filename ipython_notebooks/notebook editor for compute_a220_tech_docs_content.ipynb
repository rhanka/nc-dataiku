{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-containerized-venv-markitdown-scw-fa",
      "display_name": "Python in SCW-FA (env markitdown)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.11.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "createdOn": 1735315006284,
    "associatedRecipe": "compute_a220_tech_docs_content",
    "customFields": {},
    "dkuGit": {
      "lastInteraction": 0
    },
    "creator": "fabien.antoine@cgi.com",
    "tags": [
      "recipe-editor"
    ],
    "creationTag": {
      "versionNumber": 0,
      "lastModifiedBy": {
        "login": "fabien.antoine@cgi.com"
      },
      "lastModifiedOn": 1735315006284
    },
    "modifiedBy": "fabien.antoine@cgi.com"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nimport io\nimport pandas as pd\nimport re\nimport os\nimport json\nfrom math import ceil\n\n# Définir les dossiers d\u0027entrée et de sortie\npdf_folder \u003d dataiku.Folder(\"W8lS5GmB\")  # Dossier contenant les PDF originaux\nmd_folder \u003d dataiku.Folder(\"d7DdDueY\")   # Dossier contenant les annotations générées\n\n# Lister les fichiers PDF\npdf_files \u003d [f for f in pdf_folder.list_paths_in_partition() if f.lower().endswith(\".pdf\")]\npdf_files.sort()\n\n# Taille des lots\nBATCH_SIZE \u003d 10\n\n# Fonction pour lire le contenu d\u0027un fichier s\u0027il existe\ndef read_file_content(folder, file_path):\n    if file_path in folder.list_paths_in_partition():\n        with folder.get_download_stream(file_path) as stream:\n            return io.BytesIO(stream.read()).read().decode(\"utf-8\")\n    return None\n\n# Accéder au dataset de sortie\noutput_dataset \u003d dataiku.Dataset(\"a220_tech_docs_annotations\")\n\n# Diviser les fichiers en lots\ntotal_batches \u003d ceil(len(pdf_files) / BATCH_SIZE)\nprint(f\"Traitement de {len(pdf_files)} documents en {total_batches} lots de {BATCH_SIZE} documents.\")\n\nfor batch_num in range(total_batches):\n    start_idx \u003d batch_num * BATCH_SIZE\n    end_idx \u003d min((batch_num + 1) * BATCH_SIZE, len(pdf_files))\n    current_batch \u003d pdf_files[start_idx:end_idx]\n    \n    print(f\"Traitement du lot {batch_num + 1}/{total_batches}, documents {start_idx + 1} à {end_idx}...\")\n    \n    # Initialiser le dictionnaire pour stocker les données du lot\n    data \u003d {\n        \"doc\": [],             # Nom complet du PDF avec la page\n        \"doc_root\": [],        # Nom du PDF sans la page\n        \"json\": [],            # Contenu JSON\n        \"md\": [],              # Contenu Markdown\n        \"md_img\": [],          # Contenu Markdown avec descriptions d\u0027images\n        \"json_img\": []         # Contenu JSON avec descriptions d\u0027images\n    }\n    \n    # Colonnes dynamiques pour les images et descriptions\n    dynamic_columns \u003d set()\n\n    # Pour chaque PDF dans le lot, extraire toutes les annotations associées\n    for pdf_file in current_batch:\n        # Extraire le nom de base du document (sans l\u0027extension)\n        base_name \u003d os.path.splitext(pdf_file)[0]\n        doc_root \u003d base_name.split(\u0027_page_\u0027)[0] if \u0027_page_\u0027 in base_name else base_name\n        \n        # Chemins des fichiers d\u0027annotation\n        json_file \u003d base_name + \".json\"\n        md_file \u003d base_name + \".md\"\n        md_img_file \u003d base_name + \"__with_img_desc.md\"\n        json_img_file \u003d base_name + \"__with_img_desc.json\"\n        \n        # Lire le contenu des fichiers d\u0027annotation\n        json_content \u003d read_file_content(md_folder, json_file)\n        md_content \u003d read_file_content(md_folder, md_file)\n        md_img_content \u003d read_file_content(md_folder, md_img_file)\n        json_img_content \u003d read_file_content(md_folder, json_img_file)\n        \n        # Identifier les images associées au document\n        img_pattern \u003d re.compile(rf\"^{re.escape(base_name)}-img-(\\d+)\\.jpeg$\")\n        all_files \u003d md_folder.list_paths_in_partition()\n        \n        # Créer un dictionnaire pour stocker les informations sur les images\n        images \u003d {}\n        \n        # Chercher toutes les images et leurs descriptions\n        for file_path in all_files:\n            img_match \u003d img_pattern.match(file_path)\n            if img_match:\n                img_num \u003d img_match.group(1)\n                img_key \u003d f\"img-{img_num}\"\n                desc_file \u003d f\"{base_name}-img-{img_num}.md\"\n                \n                # Ajouter l\u0027image et sa description au dictionnaire\n                if img_key not in images:\n                    images[img_key] \u003d {\n                        \"path\": file_path,\n                        \"desc\": None\n                    }\n                \n                # Vérifier si une description existe pour cette image\n                if desc_file in all_files:\n                    img_desc \u003d read_file_content(md_folder, desc_file)\n                    images[img_key][\"desc\"] \u003d img_desc\n        \n        # Ajouter les données de base du document\n        row \u003d {\n            \"doc\": pdf_file,\n            \"doc_root\": doc_root + \".pdf\",\n            \"json\": json_content,\n            \"md\": md_content,\n            \"md_img\": md_img_content,\n            \"json_img\": json_img_content\n        }\n        \n        # Ajouter les informations sur les images\n        for img_key, img_info in sorted(images.items()):\n            row[img_key] \u003d img_info[\"path\"]\n            dynamic_columns.add(img_key)\n            \n            if img_info[\"desc\"]:\n                desc_key \u003d f\"{img_key}-desc\"\n                row[desc_key] \u003d img_info[\"desc\"]\n                dynamic_columns.add(desc_key)\n        \n        # Ajouter toutes les colonnes de base au dictionnaire de données\n        for key in data:\n            if key in row:\n                data[key].append(row[key])\n            else:\n                data[key].append(None)\n        \n        # Ajouter les colonnes dynamiques\n        for col in dynamic_columns:\n            if col not in data:\n                data[col] \u003d [None] * len(data[\"doc\"])\n            else:\n                # S\u0027assurer que la colonne a la bonne longueur\n                while len(data[col]) \u003c len(data[\"doc\"]) - 1:\n                    data[col].append(None)\n                \n            # Ajouter la valeur pour la ligne courante\n            if col in row:\n                data[col].append(row[col])\n            else:\n                data[col].append(None)\n    \n    # Créer le DataFrame pour ce lot\n    df_batch \u003d pd.DataFrame.from_dict(data)\n    \n    # Écrire le lot dans le dataset\n    if batch_num \u003d\u003d 0:\n        # Premier lot : écrire avec schéma\n        output_dataset.write_with_schema(df_batch)\n    else:\n        # Lots suivants : ajouter au dataset existant\n        output_dataset.append(df_batch)\n    \n    print(f\"Lot {batch_num + 1} traité : {len(df_batch)} documents ajoutés au dataset.\")\n\nprint(f\"Extraction terminée avec succès. Total : {len(pdf_files)} documents traités en {total_batches} lots.\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}