{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-venv-markitdown",
      "display_name": "Python (env markitdown)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "customFields": {},
    "creator": "fabien.antoine@cgi.com",
    "modifiedBy": "fabien.antoine@cgi.com",
    "createdOn": 1735622424502,
    "tags": []
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nfrom langchain.chains.question_answering import load_qa_chain\nfrom dataiku.langchain.dku_llm import DKUChatLLM\nimport json\nKB_IDs \u003d {\n    \"tech_docs\": \"zQ92IhQ9\",\n    \"non_conformities\": \"WnKb6p17\"\n}\n\nclient \u003d dataiku.api_client()\nproject \u003d client.get_default_project()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Listing available LLMs\nllm_list \u003d project.list_llms()\n\nfor llm in llm_list:\n    print(f\"- {llm.description} (id: {llm.id})\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fill with your LLM id\nLLM_ID \u003d \"openai:OpenAI-FA:gpt-4o-mini\""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Preparing the Knowledge Bank, Vector store and LLM\nKBs \u003d {\n    key: dataiku.KnowledgeBank(id\u003dvalue, project_key\u003dproject.project_key)\n    for key, value in KB_IDs.items()\n}\nvector_stores \u003d {\n    key: value.as_langchain_vectorstore()\n    for key, value in KBs.items()\n}\n\nlangchain_llm \u003d DKUChatLLM(llm_id\u003dLLM_ID, temperature\u003d0)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create the question answering chain\nchain \u003d load_qa_chain(langchain_llm, chain_type\u003d\"stuff\")\nuser_message \u003d \"Fuel Voltage Levels Quality\"\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "prompt \u003d (\n    f\"You\u0027re supporting Quality Controller for A220 and rely on the knowledge from the A220 technical \"\n    f\"doc and non conformity knowledge base (vector databases). You must provide an optimized expanded prompt towards \"\n    f\"those vector databases to enable the best retrieval given the user input. \"\n    f\"The expansion should only concern specificity around the user query and avoid retrieval of non specific vocabulary, \"\n    f\"as knowledge databses will contain any past non conformity. Avoid generic vocabulary like \u0027non-conformity\u0027, \u0027issue\u0027, \"\n    f\"\u0027specification\u0027, \u0027standard\u0027, \u0027operations\u0027, \u0027maintainance\u0027. But expand domain vocabulary.\\n \"\n    f\"Format of the output: Please just provide the query without any comment to be reused as is. \"\n    f\"Optimal request should be between 20 and 50 words \\n\\n\"\n    f\"The user is the following:\\n {user_message}\\n\\n\\n\"\n    f\"Remember to only provide the requested query for the knowledge database without any comment.\"\n)\nllm \u003d project.get_llm(LLM_ID)\ncompletion \u003d llm.new_completion()\ncompletion.with_message(prompt)\nresp \u003d completion.execute()\n\nprint(resp.text)\nquery \u003d resp.text"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search_results \u003d [result for key, value in vector_stores.items() for result in value.similarity_search(query)]\nsearch_results \u003d [ {\n        \"doc\": s.metadata[\u0027doc\u0027],\n        \"chunk_id\": s.metadata[\u0027chunk_id\u0027],\n        #\"chunk\": s.page_content\n    }\n    for s in search_results\n]\nprint(search_results)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "search_results \u003d {\n    key: value.similarity_search(query)\n    for key, value in vector_stores.items()\n}\n\nfor key in KB_IDs:\n    for search_result in search_results[key]:\n        print(f\"# {search_result.doc} \\n{search_result.page_content}\\n\")"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "        # 3rd step : give the best advice given the documents\n        \nprompt \u003d \"\"\"\n    #Processus\n    Une non conformité de l\u0027A220 doit être traitée selon le processus suivant :\n\n    000 - rapport de non-conformité par le Quality Controler\n    100 - analyse et recommandation / plan d\u0027action par le Design Office\n    200 - validation de l\u0027analyse / plan d\u0027action par le Design Manager\n    300 - calcul de structure lié au plan d\u0027action et recommandation / selon le Stress Office\n    400 - du calcul / plan d\u0027action amendé par le Stress Manager\n    500 - plan d\u0027action final validé par le Quality Manager\n\n    Vous supportez le role de l\u0027étape {000} et devez rédiger de la facon la plus explicite en prenant\n    les exemples fournis et la documentation technique.\n\n    #Exemples et documentation technique:\n    {json.dumps(search_results)}\n\n    #La requête utilisateur est la suivante:\n    {user_message}\n\n    #Réponse\n    ## Instructions de réponse    \n    Veuillez répondre pour l\u0027étape {role}, en fournissant le meilleur \u0027label\u0027 et la meilleure \u0027description\u0027 possible selon les exemples, n\u0027hésitant pas à illustrer selon les\n    documentation technique le cas échéant. La description fournie doit être complètement rédigée.\n    \n    ##Format de réponse\n    Répondez en anglais sauf si l\u0027utilisateur utilise une autre langue ou précise des instructions de langue.\n    Format de réponse attendu en json sans autre mise en forme : \u0027##(label)\\ndescription\u0027\n\"\"\"\n\ncompletion \u003d llm.new_completion()\ncompletion.with_message(prompt)\nresp \u003d completion.execute()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "deep_chat_response \u003d {\n    \"text\": resp.text,\n    \"sources\": search_results,\n    \"role\": \"ai\"\n}\nprint(deep_chat_response)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}