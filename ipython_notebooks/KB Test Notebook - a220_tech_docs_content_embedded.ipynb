{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-venv-markitdown",
      "display_name": "Python (env markitdown)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "customFields": {},
    "creator": "fabien.antoine@cgi.com",
    "modifiedBy": "fabien.antoine@cgi.com",
    "createdOn": 1735622424502,
    "tags": []
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nfrom langchain.chains.question_answering import load_qa_chain\nfrom dataiku.langchain.dku_llm import DKUChatLLM\nimport json\nKB_IDs \u003d {\n    \"tech_docs\": \"zQ92IhQ9\",\n    \"non_conformities\": \"WnKb6p17\"\n}\n\nclient \u003d dataiku.api_client()\nproject \u003d client.get_default_project()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Listing available LLMs\nllm_list \u003d project.list_llms()\n\nfor llm in llm_list:\n    print(f\"- {llm.description} (id: {llm.id})\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fill with your LLM id\nLLM_ID \u003d \"openai:OpenAI-FA:gpt-4o-mini\""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Preparing the Knowledge Bank, Vector store and LLM\nKBs \u003d {\n    key: dataiku.KnowledgeBank(id\u003dvalue, project_key\u003dproject.project_key)\n    for key, value in KB_IDs.items()\n}\nvector_stores \u003d {\n    key: value.as_langchain_vectorstore()\n    for key, value in KBs.items()\n}\n\nlangchain_llm \u003d DKUChatLLM(llm_id\u003dLLM_ID, temperature\u003d0)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create the question answering chain\nchain \u003d load_qa_chain(langchain_llm, chain_type\u003d\"stuff\")\nuser_message \u003d \"Fuel Voltage Levels Quality\"\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "prompt \u003d (\n    f\"You\u0027re supporting Quality Controller for A220 and rely on the knowledge from the A220 technical \"\n    f\"doc and non conformity knowledge base (vector databases). You must provide an optimized expanded prompt towards \"\n    f\"those vector databases to enable the best retrieval given the user input. \"\n    f\"The expansion should only concern specificity around the user query and avoid retrieval of non specific vocabulary, \"\n    f\"as knowledge databses will contain any past non conformity. Avoid generic vocabulary like \u0027non-conformity\u0027, \u0027issue\u0027, \"\n    f\"\u0027specification\u0027, \u0027standard\u0027, \u0027operations\u0027, \u0027maintainance\u0027. But expand domain vocabulary.\\n \"\n    f\"Format of the output: Please just provide the query without any comment to be reused as is. \"\n    f\"Optimal request should be between 20 and 50 words \\n\\n\"\n    f\"The user is the following:\\n {user_message}\\n\\n\\n\"\n    f\"Remember to only provide the requested query for the knowledge database without any comment.\"\n)\nllm \u003d project.get_llm(LLM_ID)\ncompletion \u003d llm.new_completion()\ncompletion.with_message(prompt)\nresp \u003d completion.execute()\n\nprint(resp.text)\nquery \u003d resp"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search_results \u003d [result for key, value in vector_stores.items() for result in value.similarity_search(query)]\nsearch_results \u003d [ {\n        \"doc\": s.metadata[\u0027doc\u0027],\n        \"chunk_id\": s.metadata[\u0027chunk_id\u0027],\n        #\"chunk\": s.page_content\n    }\n    for s in search_results\n]\nprint(search_results)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": false
      },
      "source": [
        "search_results \u003d {\n    key: value.similarity_search(query)\n    for key, value in vector_stores.items()\n}\n\nfor key in KB_IDs:\n    for search_result in search_results[key]:\n        print(f\"# {search_result.doc} \\n{search_result.page_content}\\n\")"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(dict(search_result))"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ⚡ Get the results ⚡\nresp \u003d chain({\"input_documents\":search_results, \"question\": query})\nprint(resp[\"output_text\"])"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}