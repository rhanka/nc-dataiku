{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-containerized-venv-markitdown-scw-fa",
      "display_name": "Python in SCW-FA (env markitdown)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.11.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "createdOn": 1738685410316,
    "creator": "ludovic.bocken@cgi.com",
    "associatedRecipe": "compute_named_entity_recognition_with_nltk",
    "tags": [
      "recipe-editor"
    ],
    "customFields": {},
    "modifiedBy": "ludovic.bocken@cgi.com"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\nimport nltk\nfrom nltk import word_tokenize, pos_tag, ne_chunk\nfrom nltk.tree import Tree\n\n# Ensure you have the necessary NLTK data files\nnltk.download(\u0027punkt\u0027)\nnltk.download(\u0027punkt_tab\u0027)\nnltk.download(\u0027maxent_ne_chunker\u0027)\nnltk.download(\u0027maxent_ne_chunker_tab\u0027)\n\nnltk.download(\u0027words\u0027)\nnltk.download(\u0027averaged_perceptron_tagger\u0027)\nnltk.download(\u0027averaged_perceptron_tagger_eng\u0027)\n  \u003e\u003e\u003e nltk.download(\u0027maxent_ne_chunker_tab\u0027)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Read recipe inputs\nA220_tech_docs_text \u003d dataiku.Folder(\"rhnW9xGx\")\nA220_tech_docs_text_info \u003d A220_tech_docs_text.get_info()\n\n# Assuming the folder contains text files, read them into a DataFrame\nfile_paths \u003d A220_tech_docs_text.list_paths_in_partition()\ntexts \u003d []\nfor file_path in file_paths:\n    with A220_tech_docs_text.get_download_stream(file_path) as f:\n        texts.append(f.read().decode(\u0027utf-8\u0027))"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a DataFrame\ndf \u003d pd.DataFrame({\u0027text\u0027: texts})\n\n# Perform named entity recognition\ndef extract_entities(text):\n    tokens \u003d word_tokenize(text)\n    pos_tags \u003d pos_tag(tokens)\n    chunks \u003d ne_chunk(pos_tags)\n    entities \u003d []\n    for chunk in chunks:\n        if isinstance(chunk, Tree):\n            entity \u003d \" \".join([token for token, pos in chunk.leaves()])\n            entity_label \u003d chunk.label()\n            entities.append((entity, entity_label))\n    return entities\n\ndf[\u0027entities\u0027] \u003d df[\u0027text\u0027].apply(extract_entities)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n\n# Convert the DataFrame to the required format\nnamed_entity_recognition_df \u003d df.explode(\u0027entities\u0027).dropna().reset_index(drop\u003dTrue)\nnamed_entity_recognition_df[[\u0027entity\u0027, \u0027label\u0027]] \u003d pd.DataFrame(named_entity_recognition_df[\u0027entities\u0027].tolist(), index\u003dnamed_entity_recognition_df.index)\nnamed_entity_recognition_df \u003d named_entity_recognition_df.drop(columns\u003d[\u0027entities\u0027])\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Write recipe outputs\nnamed_entity_recognition \u003d dataiku.Dataset(\"named_entity_recognition_with_nltk\")\nnamed_entity_recognition.write_with_schema(named_entity_recognition_df)"
      ],
      "outputs": []
    }
  ]
}