{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python in SCW-FA (env markitdown)",
      "language": "python",
      "name": "py-dku-containerized-venv-markitdown-scw-fa"
    },
    "associatedRecipe": "compute_named_entity_recognition_with_nltk",
    "creator": "ludovic.bocken@cgi.com",
    "createdOn": 1738685410316,
    "tags": [
      "recipe-editor"
    ],
    "customFields": {}
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import dataiku\n",
        "import pandas as pd, numpy as np\n",
        "from dataiku import pandasutils as pdu\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.tree import Tree\n",
        "\n",
        "# Ensure you have the necessary NLTK data files\n",
        "nltk.download(\u0027punkt\u0027)\n",
        "nltk.download(\u0027maxent_ne_chunker\u0027)\n",
        "nltk.download(\u0027words\u0027)\n",
        "nltk.download(\u0027averaged_perceptron_tagger\u0027)\n",
        "\n",
        "# Read recipe inputs\n",
        "A220_tech_docs_text \u003d dataiku.Folder(\"rhnW9xGx\")\n",
        "A220_tech_docs_text_info \u003d A220_tech_docs_text.get_info()\n",
        "\n",
        "# Assuming the folder contains text files, read them into a DataFrame\n",
        "file_paths \u003d A220_tech_docs_text.list_paths_in_partition()\n",
        "texts \u003d []\n",
        "for file_path in file_paths:\n",
        "    with A220_tech_docs_text.get_download_stream(file_path) as f:\n",
        "        texts.append(f.read().decode(\u0027utf-8\u0027))\n",
        "\n",
        "# Create a DataFrame\n",
        "df \u003d pd.DataFrame({\u0027text\u0027: texts})\n",
        "\n",
        "# Perform named entity recognition\n",
        "def extract_entities(text):\n",
        "    tokens \u003d word_tokenize(text)\n",
        "    pos_tags \u003d pos_tag(tokens)\n",
        "    chunks \u003d ne_chunk(pos_tags)\n",
        "    entities \u003d []\n",
        "    for chunk in chunks:\n",
        "        if isinstance(chunk, Tree):\n",
        "            entity \u003d \" \".join([token for token, pos in chunk.leaves()])\n",
        "            entity_label \u003d chunk.label()\n",
        "            entities.append((entity, entity_label))\n",
        "    return entities\n",
        "\n",
        "df[\u0027entities\u0027] \u003d df[\u0027text\u0027].apply(extract_entities)\n",
        "\n",
        "# Convert the DataFrame to the required format\n",
        "named_entity_recognition_df \u003d df.explode(\u0027entities\u0027).dropna().reset_index(drop\u003dTrue)\n",
        "named_entity_recognition_df[[\u0027entity\u0027, \u0027label\u0027]] \u003d pd.DataFrame(named_entity_recognition_df[\u0027entities\u0027].tolist(), index\u003dnamed_entity_recognition_df.index)\n",
        "named_entity_recognition_df \u003d named_entity_recognition_df.drop(columns\u003d[\u0027entities\u0027])\n",
        "\n",
        "# Write recipe outputs\n",
        "named_entity_recognition \u003d dataiku.Dataset(\"named_entity_recognition_with_nltk\")\n",
        "named_entity_recognition.write_with_schema(named_entity_recognition_df)"
      ]
    }
  ]
}